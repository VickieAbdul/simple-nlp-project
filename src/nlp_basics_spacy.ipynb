{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsLkZ2Ac4E_X",
        "outputId": "b448706d-d246-4ac4-f49c-6694a4b23fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the english language pre-trained spacy model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "zqvH_hul4UtH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Machine learning is transforming the world of data science. It helps in making accurate predictions and decisions.\""
      ],
      "metadata": {
        "id": "uX-6y53u4u7V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Remove stop words using NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(text)\n",
        "filtered_text = ' '.join([word for word in word_tokens if word.lower() not in stop_words])"
      ],
      "metadata": {
        "id": "Et8fiWrAJ4qH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove the stop words, you need to apply the word tokenizer to separate the text word by word, then join the text back after removing the stopwords.\n",
        "\n",
        "We are removing the stopwords first so that we work with only the important part of the text we need."
      ],
      "metadata": {
        "id": "UEvVnxh0KPQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the filtered text through the pipeline\n",
        "doc = nlp(filtered_text)"
      ],
      "metadata": {
        "id": "DRP1NMYo42TG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence tokenize\n",
        "for sent in doc.sents:\n",
        "  print(sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkKNr09Y5HQV",
        "outputId": "8b35e7e2-4faf-4f6a-88bb-5190275b43f6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning transforming world data science .\n",
            "helps making accurate predictions decisions .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenize\n",
        "for word in doc.text:\n",
        "  print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEH4r0T47xDD",
        "outputId": "4a85ec2a-3f8f-4d7c-aa84-488792423947"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M\n",
            "a\n",
            "c\n",
            "h\n",
            "i\n",
            "n\n",
            "e\n",
            " \n",
            "l\n",
            "e\n",
            "a\n",
            "r\n",
            "n\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "t\n",
            "r\n",
            "a\n",
            "n\n",
            "s\n",
            "f\n",
            "o\n",
            "r\n",
            "m\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "w\n",
            "o\n",
            "r\n",
            "l\n",
            "d\n",
            " \n",
            "d\n",
            "a\n",
            "t\n",
            "a\n",
            " \n",
            "s\n",
            "c\n",
            "i\n",
            "e\n",
            "n\n",
            "c\n",
            "e\n",
            " \n",
            ".\n",
            " \n",
            "h\n",
            "e\n",
            "l\n",
            "p\n",
            "s\n",
            " \n",
            "m\n",
            "a\n",
            "k\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "a\n",
            "c\n",
            "c\n",
            "u\n",
            "r\n",
            "a\n",
            "t\n",
            "e\n",
            " \n",
            "p\n",
            "r\n",
            "e\n",
            "d\n",
            "i\n",
            "c\n",
            "t\n",
            "i\n",
            "o\n",
            "n\n",
            "s\n",
            " \n",
            "d\n",
            "e\n",
            "c\n",
            "i\n",
            "s\n",
            "i\n",
            "o\n",
            "n\n",
            "s\n",
            " \n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply lemmatization and placing the words side by side\n",
        "for token in doc:\n",
        "  print(token.text, '|', token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcUIW7ou8rdO",
        "outputId": "25f521b1-4538-4c17-fe22-75f14c03c1ce"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine | machine\n",
            "learning | learning\n",
            "transforming | transform\n",
            "world | world\n",
            "data | datum\n",
            "science | science\n",
            ". | .\n",
            "helps | help\n",
            "making | make\n",
            "accurate | accurate\n",
            "predictions | prediction\n",
            "decisions | decision\n",
            ". | .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see here that Spacy's lemmatization works better at returning words to a meaningful base form."
      ],
      "metadata": {
        "id": "INbK7rIONsfD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AGCkiq7N78q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}